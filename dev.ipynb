{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = open(\"voa_fa_2003-2008_orig.txt\", \"r\", encoding=\"utf-8\")\n",
    "text = reader.read()\n",
    "reader.close()\n",
    "# delete lines starting with #\n",
    "text = \"\\n\".join([line for line in text.split(\"\\n\") if not line.startswith(\"#\")])\n",
    "# delete lines with only \\n\n",
    "text = \"\\n\".join([line for line in text.split(\"\\n\") if line.strip() != \"\"])\n",
    "# save to file\n",
    "with open(\"voa_fa_2003-2008_clean.txt\", \"w\", encoding=\"utf-8\") as writer:\n",
    "    writer.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = open(\"voa_fa_2003-2008_clean.txt\", \"r\", encoding=\"utf-8\")\n",
    "text = reader.read()\n",
    "reader.close()\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = open(\"text.txt\", \"r\", encoding=\"utf-8\")\n",
    "text = reader.read()\n",
    "reader.close()\n",
    "tokens = text.encode(\"utf-8\")\n",
    "tokens = list(map(int, tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[216, 179, 216, 177, 216, 178, 217, 133, 219, 140, 217, 134]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سرزمین ایران با ده \n"
     ]
    }
   ],
   "source": [
    "# decode to persian to byte string\n",
    "\n",
    "text = bytes(tokens[:34])\n",
    "text = text.decode(\"utf-8\", errors=\"replace\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def count_pairs(tokens: list[int]) -> dict[tuple, int]:\n",
    "    \"\"\"\n",
    "    Return the count of pairs in tokens.\n",
    "    \"\"\"\n",
    "    pairs = {}\n",
    "    for i in range(len(tokens) - 1):\n",
    "        pair = (tokens[i], tokens[i + 1])\n",
    "        pairs[pair] = pairs.get(pair, 0) + 1\n",
    "    return pairs\n",
    "\n",
    "# pairs = count_pairs(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_min_pair(pairs: dict, merges: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Find the pair from a dictionary of pairs that has the smallest value.\n",
    "    \n",
    "    Args:\n",
    "        pairs (dict): A dictionary where keys are pairs and values are their associated values.\n",
    "    \n",
    "    Returns:\n",
    "        The pair with the smallest value.\n",
    "    \"\"\"\n",
    "    # Initialize variables to track the minimum pair and its associated value\n",
    "    min_pair = None\n",
    "    min_value = float(\"inf\")\n",
    "    \n",
    "    # Iterate through each pair in the pairs dictionary\n",
    "    for pair in pairs:\n",
    "        # Get the value from the merges dictionary, defaulting to infinity if not found\n",
    "        value = merges.get(pair, float(\"inf\"))\n",
    "        \n",
    "        # Check if the current value is less than the minimum value found so far\n",
    "        if value < min_value:\n",
    "            min_value = value\n",
    "            min_pair = pair\n",
    "            \n",
    "    return min_pair\n",
    "\n",
    "\n",
    "def find_max_pair(pairs: dict) -> tuple:\n",
    "    \"\"\"\n",
    "    Find the pair from a dictionary of pairs that has the largest value.\n",
    "    \n",
    "    Args:\n",
    "        pairs (dict): A dictionary where keys are pairs and values are their associated values.\n",
    "    \n",
    "    Returns:\n",
    "        The pair with the largest value.\n",
    "    \"\"\"\n",
    "    # Start with the first pair as the current best\n",
    "    best_pair = None\n",
    "    best_value = float(\"-inf\")  # Start with the smallest possible value\n",
    "    \n",
    "    # Check each pair in the dictionary\n",
    "    for pair, value in pairs.items():\n",
    "        # If this pair has a larger value than the current best, update\n",
    "        if value > best_value:\n",
    "            best_value = value\n",
    "            best_pair = pair\n",
    "    \n",
    "    return best_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merging (32, 216) into a new token 256\n",
      "merging (216, 167) into a new token 257\n",
      "merging (219, 140) into a new token 258\n",
      "merging (216, 177) into a new token 259\n"
     ]
    }
   ],
   "source": [
    "def merge_top_pair(tokens: list[int], pair: tuple[int, int], new_byte: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Merge the top pair with new_byte\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    new_tokens = []\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] == pair[0] and tokens[i + 1] == pair[1] and i < len(tokens) - 1:\n",
    "            new_tokens.append(new_byte)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_tokens.append(tokens[i])\n",
    "            i += 1\n",
    "    return new_tokens\n",
    "\n",
    "vocabulary_size = 260\n",
    "num_merges = vocabulary_size - 256\n",
    "# copy the token\n",
    "\n",
    "tokens_copy = tokens.copy()\n",
    "\n",
    "merges = {}\n",
    "for i in range(num_merges):\n",
    "    pairs = count_pairs(tokens_copy)\n",
    "    # max_pair = find_max_pair(pairs)\n",
    "    max_pair = max(pairs, key=pairs.get)\n",
    "    print(f\"merging {max_pair} into a new token {256 + i}\")\n",
    "    tokens_copy = merge_top_pair(tokens_copy, max_pair, 256 + i)\n",
    "    merges[max_pair] = 256 + i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 773\n",
      "new tokens length: 628\n",
      "compression ratio: 1.23X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"new tokens length:\", len(tokens_copy))\n",
    "print(f\"compression ratio: {len(tokens) / len(tokens_copy):.2f}X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "سرزمین ایران با ده هزار سال تاریخ و تمدن، میزبان تمدن‌های کهن متعددی چون ایلام در هزارهٔ چهارم پیش از میلاد بوده است. در سدهٔ هفتم پ. م، پادشاهی ماد بخش‌های قابل‌توجهی از فلات ایران را یکپارچه کرد. در سدهٔ ششم پ. م، شاهنشاهی هخامنشی به‌دست کوروش بزرگ بنیان نهاده شد تا ایران یکی از بزرگ‌ترین امپراتوری‌های تاریخ را تشکیل دهد. در سدهٔ چهارم پ. م، اسکندر مقدونی این قلمرو را تسخیر کرد و ایران به بخشی از سرزمین‌های هلنی تبدیل شد.\n"
     ]
    }
   ],
   "source": [
    "# decoding\n",
    "\n",
    "def decode(tokens: list[int]) -> str:\n",
    "    vocabulary = {i:bytes([i]) for i in range(256)}\n",
    "    for k, v in merges.items():\n",
    "        vocabulary[v] = vocabulary[k[0]] + vocabulary[k[1]]\n",
    "\n",
    "    text_byte = b\"\".join(vocabulary[i] for i in tokens)\n",
    "    text = text_byte.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "print(decode(tokens_copy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# encoding\n",
    "\n",
    "def encode(text: str, merges: dict) -> list[int]:\n",
    "    tokens = text.encode(\"utf-8\")\n",
    "    tokens = list(map(int, tokens))\n",
    "    while len(tokens) >= 2:\n",
    "        pairs = count_pairs(tokens)\n",
    "        min_pair = find_min_pair(pairs)\n",
    "        if min_pair in merges:\n",
    "            tokens = merge_top_pair(tokens, min_pair, merges[min_pair])\n",
    "        else:\n",
    "            break\n",
    "    return tokens\n",
    "\n",
    "tokens = encode(text, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مادرتو دیدم\n"
     ]
    }
   ],
   "source": [
    "print(decode(encode(\"مادرتو دیدم\", merges)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
